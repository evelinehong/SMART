# group-attention
A pytorch implementation of the paper "Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions".

To run and train this on dataset Math23K, you simply need to :
`sh scripts/exe_post.sh fold_name_you_wish_to_save`
